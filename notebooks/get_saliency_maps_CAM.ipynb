{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To visualize Class Activation Mapping (CAM)\n",
    "This notebook visualizes the **_block5_** activations of model3D_1 trained on smth-smth data v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Info\n",
    "Please provide path to the relevant config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"../configs/config_model1_224.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import importlib\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_parser import WebmDataset\n",
    "from data_loader_av import VideoFolder\n",
    "\n",
    "from models.multi_column import MultiColumn\n",
    "from transforms_video import *\n",
    "from grad_cam_videos import GradCam\n",
    "\n",
    "from utils import load_json_config, remove_module_from_checkpoint_state_dict\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading configuration file, model definition and its path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "config = load_json_config(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Name of the model -- model3D_1_224\n",
      "=> Checkpoint path --> ../trained_models/model3D_1_224/model_best.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# set column model\n",
    "column_cnn_def = importlib.import_module(\"{}\".format(config['conv_model']))\n",
    "model_name = config[\"model_name\"]\n",
    "\n",
    "print(\"=> Name of the model -- {}\".format(model_name))\n",
    "\n",
    "# checkpoint path to a trained model\n",
    "checkpoint_path = os.path.join(\"../\", config[\"output_dir\"], config[\"model_name\"], \"model_best.pth.tar\")\n",
    "print(\"=> Checkpoint path --> {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiColumn(config['num_classes'], column_cnn_def.Model, int(config[\"column_units\"]))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "=> loaded checkpoint '../trained_models/model3D_1_224/model_best.pth.tar' (epoch 36)\n"
     ]
    }
   ],
   "source": [
    "print(\"=> loading checkpoint\")\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "checkpoint['state_dict'] = remove_module_from_checkpoint_state_dict(\n",
    "                                              checkpoint['state_dict'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "      .format(checkpoint_path, checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center crop videos during evaluation\n",
    "transform_eval_pre = ComposeMix([\n",
    "        [Scale(config['input_spatial_size']), \"img\"],\n",
    "        [torchvision.transforms.ToPILImage(), \"img\"],\n",
    "        [torchvision.transforms.CenterCrop(config['input_spatial_size']), \"img\"]\n",
    "         ])\n",
    "\n",
    "transform_post = ComposeMix([\n",
    "        [torchvision.transforms.ToTensor(), \"img\"],\n",
    "        [torchvision.transforms.Normalize(\n",
    "                   mean=[0.485, 0.456, 0.406],  # default values for imagenet\n",
    "                   std=[0.229, 0.224, 0.225]), \"img\"]\n",
    "         ])\n",
    "\n",
    "val_data = VideoFolder(root=config['data_folder'],\n",
    "                       json_file_input=config['json_data_val'],\n",
    "                       json_file_labels=config['json_file_labels'],\n",
    "                       clip_size=config['clip_size'],\n",
    "                       nclips=config['nclips_val'],\n",
    "                       step_size=config['step_size_val'],\n",
    "                       is_val=True,\n",
    "                       transform_pre=transform_eval_pre,\n",
    "                       transform_post=transform_post,\n",
    "                       get_item_id=True,\n",
    "                       )\n",
    "dict_two_way = val_data.classes_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Manually selecting a sample to load from the loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Select random sample (or specify the index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indx = np.random.randint(len(val_data))\n",
    "\n",
    "# OR, if you know the video id\n",
    "# video_id = '96257'\n",
    "# selected_indx = [x for x in range(len(val_data)) if val_data.csv_data[x].id == video_id][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id of the video sample = 96257\n",
      "True label --> 45 (Moving something up)\n"
     ]
    }
   ],
   "source": [
    "input_data, target, item_id = val_data[selected_indx]\n",
    "input_data = input_data.unsqueeze(0)\n",
    "print(\"Id of the video sample = {}\".format(item_id))\n",
    "print(\"True label --> {} ({})\".format(target, dict_two_way[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['nclips_val'] > 1:\n",
    "    input_var = list(input_data.split(config['clip_size'], 2))\n",
    "    for idx, inp in enumerate(input_var):\n",
    "        input_var[idx] = torch.autograd.Variable(inp)\n",
    "else:\n",
    "    input_var = [torch.autograd.Variable(input_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CAM Stuff\n",
    "- You can choose the class of which you want to get CAM by changing \"`target_index`\"\n",
    "- By default, it selects the most probable class !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted index chosen = 45 (Moving something up)\n",
      "Shape of CAM mask produced = (60, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "target_index = None\n",
    "\n",
    "grad_cam = GradCam(model=model,\n",
    "                   target_layer_names=[\"block5\"],\n",
    "                   class_dict=dict_two_way,\n",
    "                   use_cuda=False,\n",
    "                   input_spatial_size=config[\"input_spatial_size\"])\n",
    "input_to_model = input_var[0]\n",
    "mask, output = grad_cam(input_to_model, target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_var).squeeze(0)\n",
    "output = torch.nn.functional.softmax(output, dim=0)\n",
    "\n",
    "# compute top5 predictions\n",
    "pred_prob, pred_top5 = output.data.topk(5)\n",
    "pred_prob = pred_prob.numpy()\n",
    "pred_top5 = pred_top5.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Writing CAM images to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. CAM visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Original input data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalize_op = UnNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "input_data_unnormalised = unnormalize_op(input_to_model.data.cpu().squeeze(0))\n",
    "input_data_unnormalised = input_data_unnormalised.permute(1, 2, 3, 0).numpy()  # (16x224x224x3)\n",
    "input_data_unnormalised = np.flip(input_data_unnormalised, 3)\n",
    "\n",
    "output_images_folder_cam_combined = os.path.join(\"cam_saved_images\", str(item_id), \"combined\")\n",
    "\n",
    "output_images_folder_original = os.path.join(\"cam_saved_images\", str(item_id), \"original\")\n",
    "output_images_folder_cam = os.path.join(\"cam_saved_images\", str(item_id), \"cam\")\n",
    "\n",
    "os.makedirs(output_images_folder_cam_combined, exist_ok=True)\n",
    "os.makedirs(output_images_folder_cam, exist_ok=True)\n",
    "os.makedirs(output_images_folder_original, exist_ok=True)\n",
    "\n",
    "clip_size = mask.shape[0]\n",
    "\n",
    "RESIZE_SIZE = 224\n",
    "RESIZE_FLAG = 0\n",
    "SAVE_INDIVIDUALS = 1\n",
    "\n",
    "for i in range(clip_size):\n",
    "    input_data_img = input_data_unnormalised[i, :, :, :]\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask[i]), cv2.COLORMAP_JET)\n",
    "    if RESIZE_FLAG:\n",
    "        input_data_img = cv2.resize(input_data_img, (RESIZE_SIZE, RESIZE_SIZE))\n",
    "        heatmap = cv2.resize(heatmap, (RESIZE_SIZE, RESIZE_SIZE))\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(input_data_img)\n",
    "    cam = cam / np.max(cam)\n",
    "    combined_img = np.concatenate((np.uint8(255 * input_data_img), np.uint8(255 * cam)), axis=1)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(output_images_folder_cam_combined, \"img%02d.jpg\" % (i + 1)), combined_img)\n",
    "    if SAVE_INDIVIDUALS:\n",
    "        cv2.imwrite(os.path.join(output_images_folder_cam, \"img%02d.jpg\" % (i + 1)), np.uint8(255 * cam))\n",
    "        cv2.imwrite(os.path.join(output_images_folder_original, \"img%02d.jpg\" % (i + 1)), np.uint8(255 * input_data_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write text file with sample info, predictions and true labels\n",
    "with open(os.path.join(output_images_folder_cam_combined, \"info.txt\"), \"w\") as fp:\n",
    "    fp.write(\"Evaluation file used = {}\\n\".format(config['json_data_val']))\n",
    "    fp.write(\"Sample index = {}\\n\".format(selected_indx))\n",
    "    fp.write(\"True label --> {} ({})\\n\".format(target, dict_two_way[target]))\n",
    "    fp.write(\"\\n##Top-5 predicted labels##\\n\")\n",
    "    for i, elem in enumerate(pred_top5):\n",
    "        fp.write(\"{}: {} --> {:.2f}\\n\".format(i + 1, dict_two_way[elem], pred_prob[i] * 100))\n",
    "    fp.write(\"\\nPredicted index chosen = {} ({})\\n\".format(pred_top5[0], dict_two_way[pred_top5[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_combined_gif = os.path.join(output_images_folder_cam_combined, \"mygif.gif\")\n",
    "os.system(\"convert -delay 10 -loop 0 {}.jpg {}\".format(\n",
    "                                    os.path.join(output_images_folder_cam_combined, \"*\"),\n",
    "                                    path_to_combined_gif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid caching media(images, gifs etc.) in IPynb\n",
    "import random\n",
    "__counter__ = random.randint(0,2e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"cam_saved_images/96257/combined/mygif.gif?1571308564\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"{}?{}\">'.format(path_to_combined_gif, __counter__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id of the video sample = 96257\n",
      "True label --> 45 (Moving something up)\n",
      "\n",
      "Top-5 Predictions:\n",
      "Top 1 :== Moving something up. Prob := 69.14%\n",
      "Top 2 :== Lifting something with something on it. Prob := 27.55%\n",
      "Top 3 :== Picking something up. Prob := 1.19%\n",
      "Top 4 :== Lifting something up completely without letting it drop down. Prob := 0.53%\n",
      "Top 5 :== Moving something away from something. Prob := 0.38%\n"
     ]
    }
   ],
   "source": [
    "print(\"Id of the video sample = {}\".format(item_id))\n",
    "print(\"True label --> {} ({})\".format(target, dict_two_way[target]))\n",
    "print(\"\\nTop-5 Predictions:\")\n",
    "for i, pred in enumerate(pred_top5):\n",
    "    print(\"Top {} :== {}. Prob := {:.2f}%\".format(i + 1, dict_two_way[pred], pred_prob[i] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
